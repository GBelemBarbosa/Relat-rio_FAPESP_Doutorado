%LTeX: language=pt-BR
%!TEX root = principal.tex

\chapter{Realizações no período}\label{chp:realizacoes}

Durante a vigência deste primeiro período do projeto de doutorado (03/2025 a 02/2026), as atividades planejadas foram cumpridas, com destaque para a conclusão dos créditos obrigatórios, aprovação em exames de qualificação e participação em evento científico, além do andamento da pesquisa bibliográfica e experimental.

\section{Atividades Acadêmicas}

No âmbito das exigências do programa de Doutorado em Matemática Aplicada do IMECC/Unicamp, foram realizadas as seguintes atividades:

\begin{itemize}
    \item Coeficiente de Rendimento (CR): O aluno mantém um CR perfeito de 4,0000;
    \item Disciplinas Cursadas no Doutorado (1º Semestre de 2025):
    \begin{itemize}
        \item MT504 — Fluxos em Redes: Aprovado com conceito A;
        \item MT853 — Tópicos em Otimização: Aprovado com conceito A;
    \end{itemize}
    \item Aproveitamento de Créditos: As disciplinas obrigatórias MT401 — Análise Aplicada e MT402 — Matrizes foram aproveitadas do Mestrado (registradas como MT801 e MT802 no 2º semestre de 2025), cumprindo os requisitos de créditos obrigatórios;
    \item Total de Créditos: Foram totalizados 18 créditos no Doutorado (incluindo aproveitamentos), avançando significativamente rumo à integralização dos créditos exigidos pelo programa;
    \item Estágio Docente: Realização do Estágio de Capacitação Docente (PED C) na disciplina MS211 - Cálculo Numérico (código CD003), sob supervisão, durante o 2º semestre de 2025;
    \item Exame de Proficiência em inglês: Aprovação no Exame de Proficiência em Inglês II (Escrito), realizado em 30/05/2025;
    \item Exames de Qualificação:
    \begin{itemize}
        \item Aprovação no Exame de Qualificação na área de Análise Aplicada (MT401), realizado em 25/08/2025;
        \item Aprovação no Exame de Qualificação na área de Matrizes (MT402), realizado em 27/08/2025.
    \end{itemize}
\end{itemize}

\section{Atividades de Pesquisa}

As atividades de pesquisa concentraram-se na investigação e aprimoramento de métodos de segunda ordem para otimização compósita não convexa.

\subsection{Definição do Problema e Notação}
Considere o problema de regressão linear com ruído:
\begin{equation} \label{P} \tag{P}
    y = Ax + \epsilon,
\end{equation}
onde $y \in \mathbb{R}^n$ é o vetor resposta e $A \in \mathbb{R}^{n \times p}$ é a matriz de desenho. O objetivo é recuperar o vetor de coeficientes $x \in \mathbb{R}^p$, assumindo que a solução verdadeira é esparsa (poucos coeficientes não nulos), uma hipótese comum e desejável em cenários onde $p \gg n$.

Para impor essa esparsidade, abordamos o problema de regularização $\ell_0$:
\begin{equation} \label{R} \tag{R}
    \min_{x \in \mathbb{R}^p} \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_0,
\end{equation}
onde $\|x\|_0$ denota a pseudo-norma $\ell_0$ (cardinalidade do suporte de $x$) e $\lambda > 0$ é o parâmetro de regularização que controla o balanço entre o erro de ajuste e a esparsidade da solução.

Durante o texto, utilizamos a seguinte notação:
\begin{itemize}
    \item $[p] \coloneq \{1, 2, \ldots, p\}$;
    \item O suporte do vetor $x$ é denotado por $\text{supp}(x) = \{i \in [p] : x_i \neq 0\}$.
\end{itemize}

O problema \eqref{R} é NP-difícil \cite{fastselect}, o que motiva o desenvolvimento de algoritmos que combinem a eficiência de métodos contínuos com a qualidade de soluções de métodos combinatoriais.

Os principais avanços são descritos nas seções a seguir.

\subsection{Revisão Bibliográfica}
Estudo aprofundado de referências centrais e do estado da arte:
\begin{itemize}
    \item \textit{Sparse regression at scale: branch-and-bound rooted in first-order optimization} \cite{mio};
    \item \textit{First-Order Methods in Optimization} \cite{beckbook};
    \item \textit{Fast Best Subset Selection} \cite{fastselect}, para comparação com métodos de seleção de subconjuntos.
\end{itemize}

\section{Desenvolvimento Algorítmico e Experimental}
Foram realizados vários experimentos computacionais no ambiente \texttt{Julia}, bem como desenvolvimentos teóricos, focados na robustez e eficiência dos algoritmos para o problema de seleção de variáveis. As principais contribuições são detalhadas a seguir.

\subsection{Estratégias Avançadas de Validação Cruzada}
Implementamos e comparamos novas estratégias de Validação Cruzada (Cross-Validation - CV) para a seleção do hiperparâmetro $\lambda$ ao longo do caminho de regularização. A escolha adequada de $\lambda$ é fundamental para o desempenho do modelo, pois controla o equilíbrio entre o ajuste aos dados e a esparsidade da solução. Em cenários reais, onde o suporte verdadeiro é desconhecido, o erro de validação (erro de predição em um conjunto de dados não utilizado no treinamento) serve como critério para identificar o modelo que melhor generaliza.

Nosso objetivo com essas estratégias é mitigar a convergência para mínimos locais ruins. Essa estratégia é particularmente essencial em contextos nos quais a esparsidade da solução original não é conhecida, como em problemas de seleção de variáveis em aprendizado de máquina.

A abordagem padrão na literatura (e em pacotes como L0Learn e GLMNet) é percorrer o caminho de regularização de $\lambda_{high}$ (solução esparsa) para $\lambda_{low}$ (solução densa), utilizando a solução anterior como ponto de partida (\textit{warmup}). No entanto, em problemas não convexos como a regularização $\ell_0$, essa estratégia gananciosa pode ficar presa precocemente em mínimos locais subótimos.

Para contornar isso, a primeira modificação desenvolvida, denominada \emph{CV Inverso}, inverte a lógica padrão. Esta estratégia percorre o caminho de $\lambda_{low}$ a $\lambda_{high}$, iniciando de soluções mais densas. A intuição é que, ao permitir que mais variáveis entrem no modelo inicialmente (devido à menor penalização), o algoritmo pode explorar melhor o espaço de busca combinatorial antes de ser forçado a esparsificar a solução, potencialmente encontrando suportes de melhor qualidade que seriam ignorados pela abordagem padrão. 

Além disso, desenvolvemos o \emph{CV Adaptativo Inteligente}, uma abordagem híbrida que sonda os erros de validação nos dois extremos do intervalo de $\lambda$ e seleciona a melhor direção de varredura. O algoritmo permite ainda uma única reversão de direção caso detecte uma melhora significativa no erro de validação (maior que 1\%), permitindo explorar vales de mínimos locais que seriam ignorados por varreduras monotônicas rígidas. Esse critério também permite que o método termine mais rapidamente sem varrer por completo o intervalo de possíveis valores de $\lambda$, por vezes superando a padrão ou a inversa em velocidade.

\subsection{Integração Híbrida e Escalamento Dinâmico}
Para algoritmos baseados em passos espectrais, corriqueiramente empregados em métodos como o Gradiente Proximal Espectral Não-Monótono (NSPG) \cite{manuscrito} e sua variação de métrica variável (VMNSPG) \cite{vmspg}, a escala do passo $\gamma_{k,0}$ varia dinamicamente. Para garantir a consistência de seleção do operador proximal, implementamos um ajuste no caminho de regularização onde os valores de teste de $\lambda$ são compensados pela magnitude do passo espectral corrente. Isso assegura que o operador proximal aplique o limiar efetivo adequado, estabilizando a seleção do suporte.

Essa técnica é fundamental para a estratégia híbrida desenvolvida, que combina as forças de diferentes classes de algoritmos em dois estágios. Na fase global, utilizamos o NSPG para escapar de mínimos locais rasos e identificar rapidamente um suporte razoável. Na fase local subsequente, estratégias de \textit{Coordinate Partial Swap} (CPSI) \cite{fastselect} garantem otimalidade combinatorial de ordem superior. Também testamos o uso das soluções do NSPG como ponto de partida para o \textit{Partially Greedy Cyclic Coordinate Descent} (PGCCD) \cite{fastselect}, gerando um método híbrido. O PGCCD converge rapidamente para um mínimo de alta precisão, refinando os coeficientes no suporte identificado, antes da busca por otimalidade combinatorial.

\subsection{Resultados Preliminares}
Para validar a eficácia dos métodos, utilizamos dados sintéticos gerados segundo o modelo linear \eqref{P} definido anteriormente. A matriz de dados $A$ possui colunas normalizadas e estrutura de correlação controlada pelo parâmetro $\rho$ (correlação entre variáveis adjacentes com decaimento exponencial ou correlação constante). O nível de ruído é determinado pelo \textit{Signal to Noise Ratio} (SNR), que varia conforme a dificuldade do cenário analisado.

A qualidade da solução recuperada é medida pela métrica de Similaridade do Suporte (Jaccard Modificado), definida como $J(S, S^\dagger) = \frac{|S \cap S^\dagger|}{\max\{|S|, |S^\dagger|\}}$, onde $S$ é o suporte estimado pelo algoritmo e $S^\dagger$ é o suporte verdadeiro (com $|S^\dagger|=k^\dagger$). Esta métrica varia de 0 (disjunção total) a 1 (recuperação perfeita).

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time}
    \end{minipage}
    \caption{Comparação entre o método proposto (PGCCD/NSPG) e o estado da arte (L0Learn variants) em cenário de correlação exponencial ($\rho=0.5,\ p=2000,\ n=500,\ k^{\dagger}=100,\ \text{SNR}=10$). Os métodos propostos atingem recuperação de suporte superior ou equivalente com tempos competitivos.}
    \label{fig:results_comp_exp}
\end{figure}

Além do cenário exponencial, avaliamos o desempenho em configurações de correlação constante, que impõem desafios diferentes à estrutura de correlação das variáveis. A Figura \ref{fig:results_comp_const} apresenta os resultados para $\rho=0.9$ e $p=1000$.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim_const}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time_const}
    \end{minipage}
    \caption{Desempenho em cenário de correlação constante ($\rho=0.9,\ p=1000,\ n=250,\ k^{\dagger}=20,\ \text{SNR}=5$). Neste regime de alta correlação e ruído mais acentuado, os métodos propostos demonstram superioridade marcante na qualidade da solução recuperada em comparação com as alternativas, mantendo a eficiência computacional.}
    \label{fig:results_comp_const}
\end{figure}

Os resultados ilustrados nas Figuras \ref{fig:results_comp_exp} e \ref{fig:results_comp_const} indicam a robustez da combinação proposta. Em cenários desafiadores de ruído considerável e alta correlação entre os parâmetros — seja ela exponencial ou constante — a abordagem híbrida supera ou iguala o estado da arte (L0Learn) em termos de qualidade da solução recuperada. 

Particularmente no cenário de correlação exponencial (Fig. \ref{fig:results_comp_exp}), cabe ressaltar que, embora o híbrido NSPG+PGCCD possa ser mais lento para valores superiores de $n$, ele atinge recuperação perfeita em regimes de $n$ pequeno, onde seu tempo de execução permanece competitivo, oferecendo uma alternativa valiosa para cenários de dados limitados.
