%LTeX: language=pt-BR
%!TEX root = principal.tex

\chapter{Realizações no período}\label{chp:realizacoes}

Durante a vigência deste primeiro período do projeto de doutorado (03/2025 a 02/2026), as atividades planejadas foram cumpridas, com destaque para a conclusão dos créditos obrigatórios, aprovação em exames de qualificação e participação em evento científico, além do andamento da pesquisa bibliográfica e experimental.

\section{Atividades Acadêmicas}

No âmbito das exigências do programa de Doutorado em Matemática Aplicada do IMECC/Unicamp, foram realizadas as seguintes atividades:

\begin{itemize}
    \item Coeficiente de Rendimento (CR): O aluno mantém um CR perfeito de 4,0000;
    \item Disciplinas Cursadas no Doutorado (1º Semestre de 2025):
    \begin{itemize}
        \item MT504 - Fluxos em Redes: Aprovado com conceito A;
        \item MT853 - Tópicos em Otimização: Aprovado com conceito A;
    \end{itemize}
    \item Aproveitamento de Créditos: As disciplinas obrigatórias MT401 - Análise Aplicada e MT402 - Matrizes foram aproveitadas do Mestrado (registradas como MT801 e MT802 no 2º semestre de 2025), cumprindo os requisitos de créditos obrigatórios;
    \item Total de Créditos: Foram totalizados 18 créditos no Doutorado (incluindo aproveitamentos), avançando significativamente rumo à integralização dos créditos exigidos pelo programa;
    \item Estágio Docente: Realização do Estágio de Capacitação Docente (PED C) na disciplina MS211 - Cálculo Numérico (código CD003), sob supervisão, durante o 2º semestre de 2025;
    \item Exame de Proficiência em Inglês: Aprovação no Exame de Proficiência em Inglês II (Escrito), realizado em 30/05/2025;
    \item Exames de Qualificação:
    \begin{itemize}
        \item Aprovação no Exame de Qualificação na área de Análise Aplicada (MT401), realizado em 25/08/2025;
        \item Aprovação no Exame de Qualificação na área de Matrizes (MT402), realizado em 27/08/2025.
    \end{itemize}
\end{itemize}

\section{Atividades de Pesquisa}

As atividades de pesquisa concentraram-se na investigação e aprimoramento de métodos de segunda ordem para otimização compósita não convexa, com ênfase no problema de regularização $\ell_0$:
\begin{equation}
    \min_{x \in \mathbb{R}^n} f(x) + \lambda \|x\|_0,
\end{equation}
onde $f(x)$ é uma função suave (perda quadrática ou logística) e $\lambda > 0$ é o parâmetro de regularização.

Os principais avanços incluem:

\subsection{Revisão Bibliográfica}
Estudo aprofundado das referências centrais e do estado da arte:
\begin{itemize}
    \item \textit{Sparse regression at scale: branch-and-bound rooted in first-order optimization} \cite{mio};
    \item \textit{First-Order Methods in Optimization} \cite{beckbook};
    \item \textit{Fast Best Subset Selection} \cite{fastselect}, para comparação com métodos de seleção de subconjuntos.
\end{itemize}

\section{Desenvolvimento Algorítmico e Experimental}
Foi realizada uma extensa campanha de experimentos e desenvolvimentos teóricos no ambiente \texttt{Julia}, focada na robustez e eficiência dos algoritmos para o problema de seleção de variáveis. As principais contribuições são detalhadas a seguir.

\subsection{Estratégias Avançadas de Validação Cruzada}
Implementamos e comparamos novas estratégias para a seleção do hiperparâmetro $\lambda$ ao longo do caminho de regularização, visando mitigar a convergência para mínimos locais ruins. Essa estratégia é particularmente essencial em contextos nos quais a esparsidade da verdade base (\textit{ground truth}) não é conhecida, como em problemas de seleção de variáveis em aprendizado de máquina.

A primeira abordagem desenvolvida, denominada \emph{CV Inverso}, inverte a lógica padrão de percorrer o caminho de regularização. Ao invés de iniciar com $\lambda_{high}$ (solução esparsa) e relaxar a penalidade, esta estratégia percorre o caminho de $\lambda_{low}$ a $\lambda_{high}$, iniciando de soluções mais densas após uma fase de \textit{warmup}. Observamos que iniciar com o suporte cheio e remover variáveis progressivamente evita o custo combinatório de adicionar variáveis uma a uma e é particularmente robusto para métodos espectrais como o NSPG.

Além disso, desenvolvemos o \emph{CV Adaptativo Inteligente}, uma abordagem híbrida que sonda os erros de validação nos dois extremos do intervalo de $\lambda$ e seleciona a melhor direção de varredura. O algoritmo permite ainda uma única reversão de direção caso detecte uma melhora significativa no erro de validação (maior que 1\%), permitindo explorar vales de mínimos locais que seriam ignorados por varreduras monotônicas rígidas. Esse critério também permite que o método termine mais rapidamente sem varrer por completo o \textit{range} de possíveis valores de $\lambda$, por vezes superando a padrão ou a inversa em velocidade.

\subsection{Integração Híbrida e Escalamento Dinâmico}
Para algoritmos baseados em passos espectrais, como o NSPG e VMNSPG, a escala do passo $\gamma_{k,0}$ varia dinamicamente. Para garantir a consistência de seleção do operador proximal, implementamos um ajuste no caminho de regularização onde os valores de teste de $\lambda$ são compensados pela magnitude do passo espectral corrente. Isso assegura que o operador proximal aplique o \textit{threshold} efetivo adequado, estabilizando a seleção do suporte.

Essa técnica é fundamental para a estratégia híbrida desenvolvida, que combina as forças de diferentes classes de algoritmos em dois estágios. Na fase global, utilizamos o Gradiente Proximal Espectral Não-Monótono (NSPG) para escapar de mínimos locais rasos e identificar rapidamente um suporte razoável. Na fase local subsequente, estratégias de \textit{Coordinate Partial Swap} (CPSI) garantem otimalidade combinatorial de ordem superior. Também testamos o uso das soluções do NSPG como ponto de partida para o \textit{Partially Greedy Cyclic Coordinate Descent} (PGCCD) \cite{fastselect}, gerando um método híbrido. O PGCCD converge rapidamente para um mínimo de alta precisão, refinando os coeficientes no suporte identificado, antes da busca por otimalidade combinatorial.

\subsection{Resultados Preliminares}
Para validar a eficácia dos métodos, utilizamos dados sintéticos gerados segundo o modelo linear $y = Ax + \epsilon$, onde $n$ denota o número de amostras, $p$ o número de parâmetros (variáveis) e $k^\dagger$ o tamanho do suporte da verdade terrestre (\textit{ground truth}). A matriz de dados $A$ possui colunas normalizadas e estrutura de correlação controlada pelo parâmetro $\rho$ (correlação entre variáveis adjacentes com decaimento exponencial ou correlação constante). O nível de ruído é determinado pelo \textit{Signal to Noise Ratio} (SNR), que varia de acordo com a dificuldade do cenário analisado.

A qualidade da solução recuperada é medida pela métrica de Similaridade do Suporte (Jaccard Modificado), definida como $J(S, S^\dagger) = \frac{|S \cap S^\dagger|}{|S \cup S^\dagger|}$, onde $S$ é o suporte estimado pelo algoritmo e $S^\dagger$ é o suporte verdadeiro. Esta métrica varia de 0 (disjunção total) a 1 (recuperação perfeita).

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time}
    \end{minipage}
    \caption{Comparação entre o método proposto (PGCCD/NSPG) e o estado da arte (L0Learn variants) em cenário de correlação exponencial ($\rho=0.5, p=2000, n=500, k^{\dagger}=100, \text{SNR}=10$). Os métodos propostos atingem recuperação de suporte superior ou equivalente com tempos competitivos.}
    \label{fig:results_comp_exp}
\end{figure}

Além do cenário exponencial, avaliamos o desempenho em configurações de correlação constante, que impõem desafios diferentes à estrutura de correlação das variáveis. A Figura \ref{fig:results_comp_const} apresenta os resultados para $\rho=0.9$ e $p=1000$.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim_const}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time_const}
    \end{minipage}
    \caption{Desempenho em cenário de correlação constante ($\rho=0.9, p=1000, n=250, k^{\dagger}=20, \text{SNR}=5$). Neste regime de alta correlação e ruído mais acentuado, os métodos propostos demonstram superioridade marcante na qualidade da solução recuperada em comparação com as alternativas, mantendo a eficiência computacional.}
    \label{fig:results_comp_const}
\end{figure}

Os resultados ilustrados nas Figuras \ref{fig:results_comp_exp} e \ref{fig:results_comp_const} indicam a robustez da combinação proposta. Em cenários desafiadores de ruído considerável e alta correlação entre os parâmetros — seja ela exponencial ou constante — a abordagem híbrida supera ou iguala o estado da arte (L0Learn) em termos de qualidade da solução recuperada e tempo de execução.
