%LTeX: language=pt-BR
%!TEX root = principal.tex

\chapter{Realizações no período}\label{chp:realizacoes}

Durante a vigência deste primeiro período do projeto de doutorado (03/2025 a 02/2026), as atividades planejadas foram cumpridas, com destaque para a conclusão dos créditos obrigatórios, aprovação em exames de qualificação e participação em evento científico, além do andamento da pesquisa bibliográfica e experimental.

\section{Atividades Acadêmicas}

No âmbito das exigências do programa de Doutorado em Matemática Aplicada do IMECC/Unicamp, foram realizadas as seguintes atividades:

\begin{itemize}
    \item Coeficiente de Rendimento (CR): O aluno mantém um CR perfeito de 4,0000;
    \item Disciplinas Cursadas no Doutorado (1º Semestre de 2025):
    \begin{itemize}
        \item MT504 — Fluxos em Redes: Aprovado com conceito A;
        \item MT853 — Tópicos em Otimização: Aprovado com conceito A;
    \end{itemize}
    \item Aproveitamento de Créditos: As disciplinas obrigatórias MT401 — Análise Aplicada e MT402 — Matrizes foram aproveitadas do Mestrado (registradas como MT801 e MT802 no 2º semestre de 2025), cumprindo os requisitos de créditos obrigatórios;
    \item Total de Créditos: Foram totalizados 18 créditos no Doutorado (incluindo aproveitamentos), avançando significativamente rumo à integralização dos créditos exigidos pelo programa;
    \item Estágio Docente: Realização do Estágio de Capacitação Docente (PED C) na disciplina MS211 - Cálculo Numérico (código CD003), sob supervisão, durante o 2º semestre de 2025;
    \item Exame de Proficiência em inglês: Aprovação no Exame de Proficiência em Inglês II (Escrito), realizado em 30/05/2025;
    \item Exames de Qualificação:
    \begin{itemize}
        \item Aprovação no Exame de Qualificação na área de Análise Aplicada (MT401), realizado em 25/08/2025;
        \item Aprovação no Exame de Qualificação na área de Matrizes (MT402), realizado em 27/08/2025.
    \end{itemize}
\end{itemize}

\section{Atividades de Pesquisa}

As atividades de pesquisa concentraram-se na investigação e aprimoramento de métodos de segunda ordem para otimização compósita não convexa.

\subsection{Definição do Problema e Notação}
Considere o problema de regressão linear com ruído:
\begin{equation} \label{P} \tag{P}
    y = Ax + \epsilon,
\end{equation}
onde $y \in \mathbb{R}^n$ é o vetor resposta e $A \in \mathbb{R}^{n \times p}$ é a matriz de desenho. O objetivo é recuperar o vetor de coeficientes $x \in \mathbb{R}^p$, assumindo que a solução verdadeira é esparsa (poucos coeficientes não nulos), uma hipótese comum e desejável em cenários onde $p \gg n$.

Para impor essa esparsidade, abordamos o problema de regularização $\ell_0$:
\begin{equation} \label{R} \tag{R}
    \min_{x \in \mathbb{R}^p} \frac{1}{2} \|Ax - y\|_2^2 + \lambda \|x\|_0,
\end{equation}
onde $\|x\|_0$ denota a pseudo-norma $\ell_0$ (cardinalidade do suporte de $x$) e $\lambda > 0$ é o parâmetro de regularização que controla o balanço entre o erro de ajuste e a esparsidade da solução.

Durante o texto, utilizamos a seguinte notação:
\begin{itemize}
    \item $[p] \coloneq \{1, 2, \ldots, p\}$;
    \item O suporte do vetor $x$ é denotado por $\text{supp}(x) = \{i \in [p] : x_i \neq 0\}$.
\end{itemize}

O problema \eqref{R} é NP-difícil \cite{fastselect}, o que motiva o desenvolvimento de algoritmos que combinem a eficiência de métodos contínuos com a qualidade de soluções de métodos combinatoriais.

Os principais avanços são descritos nas seções a seguir.

\subsection{Revisão Bibliográfica}
Estudo aprofundado de referências centrais e do estado da arte:
\begin{itemize}
    \item \textit{Sparse regression at scale: branch-and-bound rooted in first-order optimization} \cite{mio};
    \item \textit{First-Order Methods in Optimization} \cite{beckbook};
    \item \textit{Fast Best Subset Selection} \cite{fastselect}, para comparação com métodos de seleção de subconjuntos.
\end{itemize}

\section{Desenvolvimento Algorítmico e Experimental}
Foram realizados vários experimentos computacionais no ambiente \texttt{Julia}, bem como desenvolvimentos teóricos, focados na robustez e eficiência dos algoritmos para o problema de seleção de variáveis. As principais contribuições são detalhadas a seguir.

\subsection{Estratégias Avançadas de Validação Cruzada}
Implementamos e comparamos novas estratégias de Validação Cruzada (Cross-Validation - CV) para a seleção do hiperparâmetro $\lambda$ ao longo do caminho de regularização. Essas estratégias foram testadas no ambiente computacional \texttt{Julia}. A escolha adequada de $\lambda$ é fundamental para o desempenho do modelo, pois controla o equilíbrio entre o ajuste aos dados e a esparsidade da solução. Em cenários reais, onde o suporte verdadeiro é desconhecido, o erro de validação (erro de predição em um conjunto de dados não utilizado no treinamento) serve como critério para identificar o modelo que melhor generaliza.

Nosso objetivo com essas estratégias é mitigar a convergência para mínimos locais ruins. Essa estratégia é particularmente essencial em contextos nos quais a esparsidade da solução original não é conhecida, como em problemas de seleção de variáveis em aprendizado de máquina.

A abordagem padrão na literatura (e em pacotes como L0Learn e GLMNet) é percorrer o caminho de regularização de $\lambda_{high}$ (solução esparsa) para $\lambda_{low}$ (solução densa), utilizando a solução anterior como ponto de partida (\textit{warmup}). No entanto, em problemas não convexos como a regularização $\ell_0$, essa estratégia gananciosa pode ficar presa precocemente em mínimos locais subótimos.

A primeira abordagem desenvolvida, denominada \emph{CV Inverso}, inverte a lógica padrão. Esta estratégia percorre o caminho de $\lambda_{\text{low}}$ a $\lambda_{\text{high}}$, iniciando de soluções mais densas. Diferente da "partida fria" (iniciar do zero), implementamos um \emph{warmup} específico: partindo de um $\lambda$ elevado, reduzimos seu valor sucessivamente (por um fator de $0,9$) até que o algoritmo encontre uma solução com suporte não-nulo ($\|x\|_0 > 0$). Esse valor é então utilizado como ponto de partida para a varredura ascendente. A intuição é que, ao permitir que mais variáveis entrem no modelo inicialmente, o algoritmo pode explorar melhor o espaço de busca combinatorial antes de ser forçado a esparsificar a solução, evitando mínimos locais associados a suportes incorretos.

Além disso, desenvolvemos o \emph{CV Adaptativo Inteligente}, uma abordagem híbrida que sonda os erros de validação nos dois extremos do intervalo ($\lambda_{\text{low}}$ e $\lambda_{\text{high}}$) e seleciona a direção de varredura que apresenta o menor erro de validação inicial. O algoritmo permite ainda uma única reversão de direção caso a varredura na direção escolhida não produza uma redução significativa no erro de validação (pelo menos $1\%$) nas primeiras etapas, permitindo explorar vales de mínimos locais que poderiam ser acessíveis apenas pelo outro extremo. Embora exista o risco teórico de não visitar o ótimo global se a superfície do erro de validação for altamente multimodal, essa heurística demonstrou, empiricamente, um balanço favorável: obteve resultados de qualidade competitiva com o \emph{Grid Search} exaustivo, porém com custo computacional reduzido pela metade, ao evitar o cômputo de caminhos de regularização pouco promissores.

\subsection{Integração Híbrida e Escala Dinâmica}
Recentemente, desenvolvemos variantes do método do Gradiente Proximal Espectral (como o NSPG \cite{manuscrito} e VMNSPG \cite{vmspg}) projetadas para lidar com a norma $\ell_0$. Visto que esses métodos apresentam convergência acelerada em comparação a abordagens de primeira ordem — sem o custo computacional de informações de segunda ordem — decidimos investigar sua eficácia em problemas práticos de seleção de variáveis. Para isso, integramos esses métodos com a estratégia de busca proposta. No entanto, sua aplicação direta impõe desafios: para algoritmos baseados em passos espectrais, corre-se o risco de instabilidade na seleção do suporte devido à variação da escala do passo.

No contexto espectral, o passo $\gamma_k$ (uma aproximação escalar para a inversa da Hessiana) varia dinamicamente a cada iteração para capturar a curvatura local da função objetivo. O operador proximal para a regularização $\ell_0$ impõe esparsidade zerando coeficientes menores que um limiar $\tau = \sqrt{2\gamma_k\lambda}$. Portanto, a agressividade do corte depende não somente do hiperparâmetro $\lambda$, mas também do passo espectral $\gamma_k$. Como $\gamma_k$ oscila, um valor fixo de $\lambda$ resultaria em um limiar de corte instável, fazendo com que variáveis entrassem e saíssem do suporte erraticamente. Para mitigar isso, propomos compensar essa variação: ajustamos o $\lambda$ efetivo utilizando informação do $\gamma_k$, visando manter o limiar de decisão estável ao longo das iterações.

Essa técnica é fundamental para a estratégia híbrida desenvolvida, que combina as forças de diferentes classes de algoritmos em dois estágios. Na fase global, utilizamos o NSPG para escapar de mínimos locais rasos e identificar rapidamente um suporte razoável. Na fase seguinte, estratégias de \textit{Coordinate Partial Swap} (CPSI) \cite{fastselect} garantem otimalidade combinatorial de ordem superior. Além disso, testamos o uso das soluções do NSPG como ponto de partida para o \textit{Partially Greedy Cyclic Coordinate Descent} (PGCCD) \cite{fastselect}, gerando um método híbrido. O PGCCD converge rapidamente para um mínimo de alta precisão, refinando os coeficientes no suporte identificado, antes da busca por otimalidade combinatorial.

\subsection{Resultados Preliminares}
Para validar a eficácia dos métodos, utilizamos dados sintéticos gerados segundo o modelo linear \eqref{P} conforme descrito em \cite{fastselect}. A matriz de dados $A$ possui colunas normalizadas e estrutura de correlação controlada pelo parâmetro $\rho$ (correlação entre variáveis adjacentes com decaimento exponencial ou correlação constante). O nível de ruído é determinado pelo \textit{Signal to Noise Ratio} (SNR), que varia conforme a dificuldade do cenário analisado.

A qualidade da solução recuperada é medida pela métrica de Similaridade do Suporte (Jaccard Modificado), definida como $J(S, S^\dagger) = \frac{|S \cap S^\dagger|}{\max\{|S|, |S^\dagger|\}}$, onde $S$ é o suporte estimado pelo algoritmo e $S^\dagger$ é o suporte verdadeiro (com $|S^\dagger|=k^\dagger$). Esta métrica varia de 0 (disjunção total) a 1 (recuperação perfeita).

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/mixed_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time}
    \end{minipage}
    \caption{Comparação entre o método proposto (PGCCD/NSPG) e o estado da arte (variantes do L0Learn) em cenário de correlação exponencial ($\rho=0.5,\ p=2000,\ n=500,\ k^{\dagger}=100,\ \text{SNR}=10$). Os métodos propostos atingem recuperação de suporte superior ou equivalente com tempos competitivos.}
    \label{fig:results_comp_exp}
\end{figure}

Além do cenário exponencial, avaliamos o desempenho em configurações de correlação constante, que impõem desafios diferentes à estrutura de correlação das variáveis. A Figura \ref{fig:results_comp_const} apresenta os resultados para $\rho=0.9$ e $p=1000$.

\begin{figure}[ht]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_sim.png}
        \caption{Similaridade do Suporte (Jaccard Modificado).}
        \label{fig:results_sim_const}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/const_time.png}
        \caption{Tempo de Execução (s).}
        \label{fig:results_time_const}
    \end{minipage}
    \caption{Desempenho em cenário de correlação constante ($\rho=0.9,\ p=1000,\ n=250,\ k^{\dagger}=20,\ \text{SNR}=5$). Neste regime de alta correlação e ruído mais acentuado, os métodos propostos demonstram superioridade marcante na qualidade da solução recuperada em comparação com as alternativas, mantendo a eficiência computacional.}
    \label{fig:results_comp_const}
\end{figure}

Os resultados ilustrados nas Figuras \ref{fig:results_comp_exp} e \ref{fig:results_comp_const} indicam a robustez da combinação proposta. Em cenários desafiadores de ruído considerável e alta correlação entre os parâmetros — seja ela exponencial ou constante — a abordagem híbrida supera ou iguala o estado da arte (L0Learn) em termos de qualidade da solução recuperada. 

Particularmente no cenário de correlação exponencial (Fig. \ref{fig:results_comp_exp}), cabe ressaltar que, embora o híbrido NSPG+PGCCD possa ser mais lento para valores superiores de $n$, ele atinge recuperação perfeita em regimes de $n$ pequeno, onde seu tempo de execução permanece competitivo, oferecendo uma alternativa valiosa para cenários de dados limitados.
